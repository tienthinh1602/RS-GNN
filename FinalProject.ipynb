{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence Recommendation System With Graph Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "import operator\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import Module, Parameter\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merged CSV files\n",
    "folder_path = \"amazon_dataset_2018\"\n",
    "expected_columns = [\"rating\", \"reviewerID\", \"product_id\", \"date\"]\n",
    "all_data = pd.DataFrame(columns=expected_columns)\n",
    "for filename in os.listdir(folder_path):\n",
    "  if filename.endswith(\".csv\"):\n",
    "    filepath = os.path.join(folder_path, filename)\n",
    "    try:\n",
    "      df = pd.read_csv(filepath)\n",
    "    except pd.errors.ParserError:\n",
    "      print(f\"Error parsing file: {filename}\")\n",
    "      continue\n",
    "    df = df[expected_columns]\n",
    "    all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "all_data.to_csv(\"merged_data.csv\", index=False)\n",
    "print(\"Files merged successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter reviewerIDs that only have 1 rating\n",
    "data = pd.read_csv(\"merged_data.csv\")\n",
    "user_vote_count = data.groupby(\"reviewerID\")[\"reviewerID\"].count().reset_index(name=\"vote_count\")\n",
    "filtered_data = data[data['reviewerID'].isin(\n",
    "    user_vote_count[user_vote_count['vote_count'] >= 2 & (user_vote_count['vote_count'] < 20)\n",
    "                    ]['reviewerID'].tolist())]\n",
    "filtered_data.to_csv(\"filtered_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data files and organize data based on reviewerID\n",
    "dataset = 'filtered_data.csv'\n",
    "print(\"-- Starting @ %ss\" % datetime.datetime.now())\n",
    "with open(dataset, \"r\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=',')\n",
    "    reviewer_id = {}\n",
    "    rating_date = {}\n",
    "    ctr = 0\n",
    "    curid = -1\n",
    "    curdate = None\n",
    "    for data in reader:        \n",
    "        revid = data['reviewerID']\n",
    "        if curdate and not curid == revid:\n",
    "            date = ''            \n",
    "            date = curdate        \n",
    "            rating_date[curid] = date\n",
    "        curid = revid\n",
    "        item = data['product_id']        \n",
    "        curdate = ''\n",
    "        curdate = data['date']\n",
    "        if revid in reviewer_id:\n",
    "            reviewer_id[revid] += [item]\n",
    "        else:\n",
    "            reviewer_id[revid] = [item]\n",
    "        ctr += 1\n",
    "    date = ''\n",
    "    date = curdate\n",
    "    rating_date[curid] = date\n",
    "print(\"-- Reading data @ %ss\" % datetime.datetime.now())\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set and test set data based on the dates of the assessments.\n",
    "dates = list(rating_date.items())\n",
    "maxdate = dates[0][1]\n",
    "for _, date in dates:\n",
    "    if int(maxdate) < int(date):\n",
    "        maxdate = date\n",
    "splitdate = 0\n",
    "splitdate = int(maxdate) - 86400 * 7 * 4 * 18\n",
    "print('Splitting date', splitdate)   \n",
    "\n",
    "tra_sess = filter(lambda x: x[1] < splitdate, map(lambda x: (x[0], int(x[1])), dates))\n",
    "tes_sess = filter(lambda x: x[1] > splitdate, map(lambda x: (x[0], int(x[1])), dates))\n",
    "\n",
    "tra_sess = sorted(tra_sess, key=operator.itemgetter(1))  \n",
    "tes_sess = sorted(tes_sess, key=operator.itemgetter(1))  \n",
    "print(len(tra_sess))  \n",
    "print(len(tes_sess))  \n",
    "print(tra_sess[:3])\n",
    "print(tes_sess[:3])\n",
    "print(\"-- Splitting train set and test set @ %ss\" % datetime.datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sequences representing products that have been reviewed by reviewerID\n",
    "item_dict = {}\n",
    "def obtian_tra():\n",
    "    train_ids = []\n",
    "    train_seqs = []\n",
    "    train_dates = []\n",
    "    item_ctr = 1\n",
    "    for s, date in tra_sess:\n",
    "        seq = reviewer_id[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "            else:\n",
    "                outseq += [item_ctr]\n",
    "                item_dict[i] = item_ctr\n",
    "                item_ctr += 1\n",
    "        if len(outseq) < 2: \n",
    "            continue\n",
    "        train_ids += [s]\n",
    "        train_dates += [date]\n",
    "        train_seqs += [outseq]\n",
    "    print(item_ctr)    \n",
    "    return train_ids, train_dates, train_seqs\n",
    "\n",
    "def obtian_tes():\n",
    "    test_ids = []\n",
    "    test_seqs = []\n",
    "    test_dates = []\n",
    "    for s, date in tes_sess:\n",
    "        seq = reviewer_id[s]\n",
    "        outseq = []\n",
    "        for i in seq:\n",
    "            if i in item_dict:\n",
    "                outseq += [item_dict[i]]\n",
    "        if len(outseq) < 2:\n",
    "            continue\n",
    "        test_ids += [s]\n",
    "        test_dates += [date]\n",
    "        test_seqs += [outseq]\n",
    "    return test_ids, test_dates, test_seqs\n",
    "\n",
    "tra_ids, tra_dates, tra_seqs = obtian_tra()\n",
    "tes_ids, tes_dates, tes_seqs = obtian_tes()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process sequences and corresponding dates of product reviews\n",
    "def process_seqs(iseqs, idates):\n",
    "    out_seqs = []\n",
    "    out_dates = []\n",
    "    labs = []\n",
    "    ids = []\n",
    "    for id, seq, date in zip(range(len(iseqs)), iseqs, idates):\n",
    "        for i in range(1, len(seq)):\n",
    "            tar = seq[-i]\n",
    "            labs += [tar]\n",
    "            out_seqs += [seq[:-i]]\n",
    "            out_dates += [date]\n",
    "            ids += [id]\n",
    "    return out_seqs, out_dates, labs, ids\n",
    "\n",
    "tr_seqs, tr_dates, tr_labs, tr_ids = process_seqs(tra_seqs, tra_dates)\n",
    "te_seqs, te_dates, te_labs, te_ids = process_seqs(tes_seqs, tes_dates)\n",
    "tra = (tr_seqs, tr_labs)\n",
    "tes = (te_seqs, te_labs)\n",
    "print(len(tr_seqs))\n",
    "print(len(te_seqs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average length of the strings in the training set and test set,\n",
    "# Store processed data\n",
    "all = 0\n",
    "for seq in tra_seqs:\n",
    "    all += len(seq)\n",
    "for seq in tes_seqs:\n",
    "    all += len(seq)\n",
    "print('avg length: ', all/(len(tra_seqs) + len(tes_seqs) * 1.0))\n",
    "\n",
    "if not os.path.exists('amz'):\n",
    "    os.makedirs('amz')\n",
    "pickle.dump(tra, open('amz/train.txt', 'wb'))\n",
    "pickle.dump(tes, open('amz/test.txt', 'wb'))\n",
    "pickle.dump(tra_seqs, open('amz/all_train_seq.txt', 'wb'))\n",
    "\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Processing input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the training data set into train data and validation data\n",
    "def split_validation(train_set, valid_portion):    \n",
    "    train_set_x, train_set_y = train_set     \n",
    "    n_samples = len(train_set_x)    \n",
    "    sidx = np.arange(n_samples, dtype='int32')    \n",
    "    np.random.shuffle(sidx)\n",
    "    n_train = int(np.round(n_samples * (1. - valid_portion)))\n",
    "\n",
    "    valid_set_x = [train_set_x[s] for s in sidx[n_train:]]\n",
    "    valid_set_y = [train_set_y[s] for s in sidx[n_train:]]\n",
    "    train_set_x = [train_set_x[s] for s in sidx[:n_train]]\n",
    "    train_set_y = [train_set_y[s] for s in sidx[:n_train]]\n",
    "    return (train_set_x, train_set_y), (valid_set_x, valid_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare input data for machine learning models\n",
    "def data_masks(all_usr_pois, item_tail):    \n",
    "    us_lens = [len(upois) for upois in all_usr_pois]    \n",
    "    len_max = max(us_lens)    \n",
    "    us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)]\n",
    "    us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens]\n",
    "    return us_pois, us_msks, len_max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare and create batches of data for use in training machine learning models\n",
    "class Data():\n",
    "    def __init__(self, data, shuffle=False, graph=None, opt=None):        \n",
    "        inputs = data[0]\n",
    "        inputs, mask, len_max = data_masks(inputs, [0])        \n",
    "        self.inputs = np.asarray(inputs)        \n",
    "        self.mask = np.asarray(mask)\n",
    "        self.len_max = len_max\n",
    "        self.targets = np.asarray(data[1])        \n",
    "        if opt.dynamic:\n",
    "            self.targets = np.asarray(data[2])        \n",
    "        self.length = len(inputs)        \n",
    "        self.shuffle = shuffle        \n",
    "        self.graph = graph\n",
    "\n",
    "    def generate_batch(self, batch_size):\n",
    "        if self.shuffle:            \n",
    "            shuffled_arg = np.arange(self.length)\n",
    "            np.random.shuffle(shuffled_arg)\n",
    "            self.inputs = self.inputs[shuffled_arg]\n",
    "            self.mask = self.mask[shuffled_arg]\n",
    "            self.targets = self.targets[shuffled_arg]        \n",
    "        n_batch = int(self.length / batch_size)\n",
    "        if self.length % batch_size != 0:\n",
    "            n_batch += 1        \n",
    "        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n",
    "        slices[-1] = slices[-1][:(self.length - batch_size * (n_batch - 1))]\n",
    "        return slices\n",
    "\n",
    "    def get_slice(self, i):        \n",
    "        inputs, mask, targets = self.inputs[i], self.mask[i], self.targets[i]\n",
    "        items, n_node, A, alias_inputs = [], [], [], []        \n",
    "        for u_input in inputs:\n",
    "            n_node.append(len(np.unique(u_input)))\n",
    "        max_n_node = np.max(n_node)\n",
    "\n",
    "        for u_input in inputs:\n",
    "            node = np.unique(u_input)\n",
    "            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
    "            u_A = np.zeros((max_n_node, max_n_node))\n",
    "            for i in np.arange(len(u_input) - 1):\n",
    "                if u_input[i + 1] == 0:\n",
    "                    break\n",
    "                u = np.where(node == u_input[i])[0][0]\n",
    "                v = np.where(node == u_input[i + 1])[0][0]\n",
    "                u_A[u][v] = 1\n",
    "\n",
    "            u_sum_in = np.sum(u_A, 0)\n",
    "            u_sum_in[np.where(u_sum_in == 0)] = 1\n",
    "            u_A_in = np.divide(u_A, u_sum_in)\n",
    "            u_sum_out = np.sum(u_A, 1)\n",
    "            u_sum_out[np.where(u_sum_out == 0)] = 1\n",
    "            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
    "            u_A = np.concatenate([u_A_in, u_A_out]).transpose()\n",
    "            A.append(u_A)\n",
    "            alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
    "        \n",
    "        return alias_inputs, A, items, mask, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# PositionEmbedding class is used to calculate and map the positions of elements in the input data string\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mPositionEmbedding\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m      3\u001b[0m     MODE_EXPAND \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMODE_EXPAND\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m     MODE_ADD \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMODE_ADD\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# PositionEmbedding class is used to calculate and map the positions of elements in the input data string\n",
    "class PositionEmbedding(nn.Module):\n",
    "    MODE_EXPAND = 'MODE_EXPAND'\n",
    "    MODE_ADD = 'MODE_ADD'\n",
    "    MODE_CONCAT = 'MODE_CONCAT'\n",
    "    def __init__(self,\n",
    "                 num_embeddings,\n",
    "                 embedding_dim,\n",
    "                 mode=MODE_ADD):\n",
    "        super(PositionEmbedding, self).__init__()\n",
    "        self.num_embeddings = num_embeddings  \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.mode = mode\n",
    "        if self.mode == self.MODE_EXPAND:\n",
    "            self.weight = nn.Parameter(torch.Tensor(num_embeddings * 2 + 1, embedding_dim))\n",
    "        else:\n",
    "            self.weight = nn.Parameter(torch.Tensor(num_embeddings, embedding_dim))\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_normal_(self.weight)\n",
    "    def forward(self, x):\n",
    "        if self.mode == self.MODE_EXPAND:\n",
    "            indices = torch.clamp(x, -self.num_embeddings, self.num_embeddings) + self.num_embeddings\n",
    "            return F.embedding(indices.type(torch.LongTensor), self.weight)\n",
    "        batch_size, seq_len = x.size()[:2]\n",
    "        embeddings = self.weight[:seq_len, :].view(1, seq_len, self.embedding_dim)\n",
    "        if self.mode == self.MODE_ADD:\n",
    "            return x + embeddings\n",
    "        if self.mode == self.MODE_CONCAT:\n",
    "            return torch.cat((x, embeddings.repeat(batch_size, 1, 1)), dim=-1)\n",
    "        raise NotImplementedError('Unknown mode: %s' % self.mode)\n",
    "    def extra_repr(self):\n",
    "        return 'num_embeddings={}, embedding_dim={}, mode={}'.format(\n",
    "            self.num_embeddings, self.embedding_dim, self.mode,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual class implements a residual module\n",
    "class Residual(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden_size = 120\n",
    "        self.d1 = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.d2 = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.dp = nn.Dropout(p=0.2)\n",
    "        self.drop = True\n",
    "    def forward(self, x):\n",
    "        residual = x  \n",
    "        x = F.relu(self.d1(x))\n",
    "        if self.drop:\n",
    "            x = self.d2(self.dp(x))\n",
    "        else:\n",
    "            x = self.d2(x)\n",
    "        out = residual + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiHeadedAttention class is used to learn complex relationships in input data\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, n_head, n_feat, dropout_rate):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert n_feat % n_head == 0\n",
    "        self.d_k = n_feat // n_head\n",
    "        self.h = n_head\n",
    "        self.linear_q = nn.Linear(n_feat, n_feat)\n",
    "        self.linear_k = nn.Linear(n_feat, n_feat)\n",
    "        self.linear_v = nn.Linear(n_feat, n_feat)\n",
    "        self.linear_out = nn.Linear(n_feat, n_feat)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, query, key, value, mask):\n",
    "        n_batch = query.size(0)\n",
    "        q = self.linear_q(query).view(n_batch, -1, self.h, self.d_k)\n",
    "        k = self.linear_k(key).view(n_batch, -1, self.h, self.d_k)\n",
    "        v = self.linear_v(value).view(n_batch, -1, self.h, self.d_k)\n",
    "        q = q.transpose(1, 2) \n",
    "        k = k.transpose(1, 2) \n",
    "        v = v.transpose(1, 2)  \n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)  \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).eq(0) \n",
    "            min_value = float(np.finfo(torch.tensor(0, dtype=scores.dtype).numpy().dtype).min)\n",
    "            # scores = scores.masked_fill(mask, min_value)\n",
    "            self.attn = torch.softmax(scores, dim=-1).masked_fill(mask, 0.0)  \n",
    "        else:\n",
    "            self.attn = torch.softmax(scores, dim=-1) \n",
    "        p_attn = self.dropout(self.attn)\n",
    "        x = torch.matmul(p_attn, v) \n",
    "        x = x.transpose(1, 2).contiguous().view(n_batch, -1, self.h * self.d_k) \n",
    "        return self.linear_out(x)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GNN class implements a propagation module on the graph\n",
    "class GNN(Module):\n",
    "    def __init__(self, hidden_size, step=1):\n",
    "        super(GNN, self).__init__()\n",
    "        self.step = step\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = hidden_size * 2\n",
    "        self.gate_size = 3 * hidden_size\n",
    "        self.w_ih = Parameter(torch.Tensor(self.gate_size, self.input_size))\n",
    "        self.w_hh = Parameter(torch.Tensor(self.gate_size, self.hidden_size))\n",
    "        self.b_ih = Parameter(torch.Tensor(self.gate_size))\n",
    "        self.b_hh = Parameter(torch.Tensor(self.gate_size))\n",
    "        self.b_iah = Parameter(torch.Tensor(self.hidden_size))\n",
    "        self.b_oah = Parameter(torch.Tensor(self.hidden_size))\n",
    "        self.linear_edge_in = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.linear_edge_out = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.linear_edge_f = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "\n",
    "    def GNNCell(self, A, hidden):\n",
    "        input_in = torch.matmul(A[:, :, :A.shape[1]], self.linear_edge_in(hidden)) + self.b_iah\n",
    "        input_out = torch.matmul(A[:, :, A.shape[1]: 2 * A.shape[1]], self.linear_edge_out(hidden)) + self.b_oah\n",
    "        inputs = torch.cat([input_in, input_out], 2)\n",
    "        gi = F.linear(inputs, self.w_ih, self.b_ih)\n",
    "        gh = F.linear(hidden, self.w_hh, self.b_hh)\n",
    "        i_r, i_i, i_n = gi.chunk(3, 2)\n",
    "        h_r, h_i, h_n = gh.chunk(3, 2)\n",
    "        resetgate = torch.sigmoid(i_r + h_r)\n",
    "        inputgate = torch.sigmoid(i_i + h_i)\n",
    "        newgate = torch.tanh(i_n + resetgate * h_n)\n",
    "        hy = hidden - inputgate * (hidden - newgate)\n",
    "        return hy\n",
    "\n",
    "    def forward(self, A, hidden):\n",
    "        for i in range(self.step):\n",
    "            hidden = self.GNNCell(A, hidden)\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph neural network (GNN) model is used for training\n",
    "class SessionGraph(Module):\n",
    "    def __init__(self, opt, n_node, len_max):\n",
    "        super(SessionGraph, self).__init__()\n",
    "        self.hidden_size = opt.hiddenSize\n",
    "        self.len_max = len_max\n",
    "        self.n_node = n_node\n",
    "        self.batch_size = opt.batchSize\n",
    "        self.nonhybrid = opt.nonhybrid\n",
    "        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n",
    "        self.gnn = GNN(self.hidden_size, step=opt.step)\n",
    "        self.linear_one = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.linear_two = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
    "        self.linear_three = nn.Linear(self.hidden_size, 1, bias=False)\n",
    "        self.linear_transform = nn.Linear(self.hidden_size * 2, self.hidden_size, bias=True)\n",
    "        self.rn = Residual()\n",
    "        self.multihead_attn = nn.MultiheadAttention(self.hidden_size, 1).cuda()\n",
    "        self.pe = PositionEmbedding(len_max, self.hidden_size)\n",
    "        self.loss_function = nn.CrossEntropyLoss()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=opt.lr, weight_decay=opt.l2)\n",
    "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=opt.lr_dc_step, gamma=opt.lr_dc)\n",
    "        self.reset_parameters()\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
    "        for weight in self.parameters():\n",
    "            weight.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def compute_scores(self, hidden, mask, self_att=True, residual=True, k_blocks=4):\n",
    "        ht = hidden[torch.arange(mask.shape[0]).long(), torch.sum(mask, 1) - 1] \n",
    "        mask_self = mask.repeat(1, mask.shape[1]).view(-1, mask.shape[1], mask.shape[1])\n",
    "        if self_att:            \n",
    "            attn_output = hidden\n",
    "            for k in range(k_blocks):\n",
    "                attn_output = attn_output.transpose(0,1)\n",
    "                attn_output, attn_output_weights = self.multihead_attn(attn_output, attn_output, attn_output)                \n",
    "                attn_output = attn_output.transpose(0,1)                \n",
    "                if residual:\n",
    "                    attn_output = self.rn(attn_output)\n",
    "            hn = attn_output[torch.arange(mask.shape[0]).long(), torch.sum(mask, 1) - 1]             \n",
    "            a = 0.52*hn + (1-0.52)*ht  \n",
    "        else:          \n",
    "            q1 = self.linear_one(ht).view(ht.shape[0], 1, ht.shape[1])  \n",
    "            q2 = self.linear_two(hidden)  \n",
    "            alpha = self.linear_three(torch.sigmoid(q1 + q2))\n",
    "            a = torch.sum(alpha * hidden * mask.view(mask.shape[0], -1, 1).float(), 1)\n",
    "            if not self.nonhybrid:\n",
    "                a = self.linear_transform(torch.cat([a, ht], 1))\n",
    "        b = self.embedding.weight[1:]  \n",
    "        scores = torch.matmul(a, b.transpose(1, 0))\n",
    "        return scores\n",
    "\n",
    "    def forward(self, inputs, A):\n",
    "        hidden = self.embedding(inputs)\n",
    "        hidden = self.gnn(A, hidden)\n",
    "        return hidden\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_to_cuda and trans_to_cpu to convert variables (tensor) between GPU device (CUDA) and CPU\n",
    "def trans_to_cuda(variable):\n",
    "    if torch.cuda.is_available():\n",
    "        return variable.cuda()\n",
    "    else:\n",
    "        return variable\n",
    "\n",
    "\n",
    "def trans_to_cpu(variable):\n",
    "    if torch.cuda.is_available():\n",
    "        return variable.cpu()\n",
    "    else:\n",
    "        return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward class performs forward propagation.\n",
    "def forward(model, i, data):\n",
    "    alias_inputs, A, items, mask, targets = data.get_slice(i)\n",
    "    alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())\n",
    "    items = trans_to_cuda(torch.Tensor(items).long())\n",
    "    A = trans_to_cuda(torch.Tensor(A).float())\n",
    "    mask = trans_to_cuda(torch.Tensor(mask).long())\n",
    "    hidden = model(items, A)\n",
    "    get = lambda i: hidden[i][alias_inputs[i]]\n",
    "    seq_hidden = torch.stack([get(i) for i in torch.arange(len(alias_inputs)).long()])\n",
    "    return targets, model.compute_scores(seq_hidden, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test, used to train and evaluate the model on the training and testing datasets\n",
    "def train_test(model, train_data, test_data):\n",
    "    model.scheduler.step()\n",
    "    print('start training: ', datetime.datetime.now())\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    slices = train_data.generate_batch(model.batch_size)\n",
    "    for i, j in zip(slices, np.arange(len(slices))):\n",
    "        model.optimizer.zero_grad()\n",
    "        targets, scores = forward(model, i, train_data)\n",
    "        targets = trans_to_cuda(torch.Tensor(targets).long())\n",
    "        loss = model.loss_function(scores, targets - 1)\n",
    "        loss.backward()\n",
    "        model.optimizer.step()\n",
    "        total_loss += loss\n",
    "        if j % int(len(slices) / 5 + 1) == 0:\n",
    "            print('[%d/%d] Loss: %.4f' % (j, len(slices), loss.item()))\n",
    "    print('\\tLoss:\\t%.3f' % total_loss)\n",
    "    print('start predicting: ', datetime.datetime.now())\n",
    "    model.eval()\n",
    "    hit, mrr = [], []\n",
    "    slices = test_data.generate_batch(model.batch_size)\n",
    "    for i in slices:\n",
    "        targets, scores = forward(model, i, test_data)\n",
    "        sub_scores = scores.topk(20)[1]\n",
    "        sub_scores = trans_to_cpu(sub_scores).detach().numpy()\n",
    "        for score, target, mask in zip(sub_scores, targets, test_data.mask):\n",
    "            hit.append(np.isin(target - 1, score))\n",
    "            if len(np.where(score == target - 1)[0]) == 0:\n",
    "                mrr.append(0)\n",
    "            else:\n",
    "                mrr.append(1 / (np.where(score == target - 1)[0][0] + 1))\n",
    "    hit = np.mean(hit) * 100\n",
    "    mrr = np.mean(mrr) * 100\n",
    "    return hit, mrr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "batch_size = 50\n",
    "hidden_size = 120\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "lr_dc = 0.1\n",
    "lr_dc_step = 3\n",
    "l2 = 1e-5\n",
    "step = 1\n",
    "patience = 10\n",
    "validation = True\n",
    "valid_portion = 0.1\n",
    "dynamic = False\n",
    "\n",
    "opt = {\n",
    "    'batch_size': batch_size,\n",
    "    'hidden_size': hidden_size,\n",
    "    'num_epochs': num_epochs,\n",
    "    'learning_rate': learning_rate,\n",
    "    'lr_dc': lr_dc,\n",
    "    'lr_dc_step': lr_dc_step,\n",
    "    'l2': l2,\n",
    "    'step': step,\n",
    "    'patience': patience,\n",
    "    'validation': validation,\n",
    "    'valid_portion': valid_portion,\n",
    "    'dynamic': dynamic\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data and test data\n",
    "train_data = pickle.load(open(\"amz/train.txt\", 'rb'))\n",
    "if opt['validation']:\n",
    "    train_data, valid_data = split_validation(train_data, opt['valid_portion'])\n",
    "    test_data = valid_data\n",
    "else:\n",
    "    test_data = pickle.load(open(\"amz/test.txt\", 'rb'))\n",
    "train_data = Data(train_data, shuffle=True, opt=opt)\n",
    "test_data = Data(test_data, shuffle=False, opt=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and evaluate performance\n",
    "n_node = 2412095\n",
    "model = SessionGraph(opt, n_node, max(len(train_data.data), len(test_data.data)))\n",
    "\n",
    "start = time.time()\n",
    "best_result = [0, 0]\n",
    "best_epoch = [0, 0]\n",
    "bad_counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    print('-------------------------------------------------------')\n",
    "    print('epoch: ', epoch)\n",
    "    # Placeholder for train_test function\n",
    "    hit, mrr = train_test(model, train_data, test_data)\n",
    "    flag = 0    \n",
    "    if hit >= best_result[0]:\n",
    "        best_result[0] = hit\n",
    "        best_epoch[0] = epoch\n",
    "        flag = 1    \n",
    "    if mrr >= best_result[1]:\n",
    "        best_result[1] = mrr\n",
    "        best_epoch[1] = epoch\n",
    "        flag = 1    \n",
    "    print('Best Result:')\n",
    "    print('\\tRecall@20:\\t%.4f\\tMMR@20:\\t%.4f\\tEpoch:\\t%d,\\t%d' % (best_result[0], best_result[1], best_epoch[0], best_epoch[1]))\n",
    "    bad_counter += 1 - flag\n",
    "    if bad_counter >= patience:\n",
    "        break\n",
    "print('-------------------------------------------------------')\n",
    "end = time.time()\n",
    "print(\"Run time: %f s\" % (end - start))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
